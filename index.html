<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Second Half of Unified Models">
  <meta name="keywords" content="Unified Models, Fluid Intelligence, GENIUS, Multimodal, Generative AI, Blog">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Second Half of Unified Models</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">The Second Half of Unified Models</h1>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="static/pdfs/GENIUS_paper.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark" disabled style="background-color: #d3d3d3; color: #666; cursor: not-allowed;">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>

              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark" disabled style="background-color: #d3d3d3; color: #666; cursor: not-allowed;">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width"> -->
        <div class="content has-text-justified">
          <p>
            Over the past two years, we have witnessed an explosion in Unified Multimodal Models (UMMs). 
            From Chameleon<sup><a href="#ref-1">[1]</a></sup>, 
            Show-o<sup><a href="#ref-2">[2]</a></sup> 
            to Transfusion<sup><a href="#ref-3">[3]</a></sup> 
            and Emu<sup><a href="#ref-4">[4]</a></sup>, 
            the field flourished with diverse architecture. 
            Until the emergence of Bagel<sup><a href="#ref-5">[5]</a></sup>, 
            architectural exploration gradually converged. 
            In this phase, benefited by the integration of understanding, we have observed remarkable gains on the generation side: UMMs exhibit enhanced reasoning capabilities, and reflect world knowledge in generated outputs. 
            The "VLM-as-Encoder" is becoming a new generative paradigm. 
            This flourishing era constitutes the "First Half" of UMM development, where the community has primarily focused on architectural. 
            On the generation side, attention has been largely directed toward capabilities such as static world knowledge, safety, and instruction adherence.
          </p>
          <p>
            However, as the marginal returns of architectural exploration diminish, we confront a pivotal inflection point. 
            With the construction of UMMs demystified, the research focus inevitably pivots toward their effective utilization and the identification of their intrinsic strengths, signaling our readiness to enter the "Second Half." 
            In contrast to the First Half's emphasis on implementation and benchmark maximization, this new phase necessitates the establishment of insightful evaluation frameworks that prioritize capabilities unique to UMMs and their adaptability to complex, real-world demands. 
            Despite the proliferation of models and soaring metrics witnessed in 2024–2025, a qualitative paradigm shift in practical creation and interaction remains elusive. 
            <strong>「The remainder of the Second Half will be elaborated upon in <a href="#second-half-details">subsequent sections</a>.」</strong>
          </p>
          <p>
            This rapid advancement invites a natural question: 
            <strong>How far are current UMMs from achieving true general intelligence regrading visual generation?</strong>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-3 has-text-centered">Defining General Intelligence: Crystallized vs. Fluid</h2> -->
        <div class="content has-text-justified">
          <p>
            Drawing from the literature<sup><a href="#ref-6">[6]</a></sup>, general intelligence can be decoupled into Crystallized Intelligence (CI)<sup><a href="#ref-7">[7]</a></sup> and Fluid Intelligence (FI)<sup><a href="#ref-8">[8]</a></sup>. 
            CI relies on recalling accumulated knowledge and learned schemas, while FI emphasizes the capacity to reason and solve problems in novel situations. 
            The former has been the core focus of the "First Half" of UMM development: through fitting massive datasets, models have acquired astonishing CI. 
            For instance, a model's ability to generate a flawless cat often stems from exposure to billions of instances during training, followed by probabilistic reproduction during inference. 
            However, real-world demands are diverse, often requiring models to adapt to contexts on the fly, which poses a significant challenge to their FI. 
            Coincidentally, our work aligns with the initial research from Shunyu Yao's team, focusing on "true" in-context learning<sup><a href="#ref-9">[9]</a></sup>, which is also the foundation of FI. 
            Our work can also be considered as the generative extension of their work.
          </p>
          <ul>
            <li>
                Crystallized Intelligence (CI): Relies on recalling accumulated knowledge and learned schemas. This has been the core focus of the "First Half". For instance, a model's ability to generate a flawless "cat" often stems from exposure to billions of instances during training.
            </li>
            <li>
                Fluid Intelligence (FI): Emphasizes the capacity to reason and solve problems in novel situations. Real-world demands are diverse, often requiring models to adapt to contexts on the fly.
            </li>
          </ul>
          <p>
            <strong>Coincidentally, our work aligns with the research focusing on "true" in-context learning, which is the foundation of Fluid Intelligence.</strong>
          </p>
          <p>
            Existing benchmarks, such as the classic ARC-Bench<sup><a href="#ref-10">[10]</a></sup>, are predominantly grounded in understanding. 
            While the importance of "System 2" thinking is frequently highlighted, it is typically discussed within comprehension contexts. 
            However, visual generation is approaching a similar inflection point. 
            The historical fixation on pixel-level fidelity may represent a bias; in the long term, understanding and generation should arguably not be treated as separate tasks.
          </p>
          <p>
            Guided by the definition of FI, 
            we do not assess whether a model can render a "more realistic dog"—an indicator of CI. 
            Instead, drawing inspiration from tasks humans perform effortlessly, 
            we translate these capabilities into generative challenges:
          </p>
          <img src="./static/images/showtask_page-0001.jpg" alt="GENIUS Benchmark Overview" style="width:100%; height:auto; margin-top: 20px;">
          <ul>
            <li>
              Implicit Pattern Induction: Humans intuitively discern latent contextual cues, such as stylistic nuances in text or textural patterns in imagery.
            </li>
            <li>
              Ad-hoc Constraint Execution: Humans adeptly reason under provisional, out-of-distribution constraints. This includes defining semantic visual operators or interpreting abstract symbols with context-specific meanings (e.g., performing arithmetic operations where objects symbolically represent numerical values).
            </li>
            <li>
              Contextual Knowledge Adaptation: This entails overcoming established priors to internalize counter-intuitive rules. For instance, given a counterfactual premise where "gravity is dictated by color", humans can effortlessly modulate their reasoning to imagine and generate content within this novel framework.
            </li>
          </ul>
        </div>
        
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            We evaluated twelve models and surprisingly found that 
            even the state-of-the-art Nano-Banana Pro failed to achieve a passing score. 
            (Please refer to the paper for specific metrics and prompts.) 
            Our evaluation focuses on three primary dimensions: 
            logical correctness, preservation of reference information, and aesthetic quality. 
            To ensure fairness and reproducibility, 
            we developed manually annotated rubrics for each test case 
            and utilized both open-source and proprietary models for evaluation.
          </p>
          
          <div class="table-container" style="overflow-x: auto;">
            <table class="table is-bordered is-hoverable is-striped is-fullwidth" style="font-size: 0.7em; text-align: center;">
              <thead>
                <tr style="background-color: #f5f5f5;">
                  <th rowspan="3" style="vertical-align: middle; min-width: 100px;">Method</th>
                  <th rowspan="3" style="vertical-align: middle;">Interleaved</th>
                  <th rowspan="3" style="vertical-align: middle;">Overall</th>
                  <th colspan="3" style="background-color: #eefcf3;">Implicit Pattern Induction</th>
                  <th colspan="6" style="background-color: #eef6fc;">Ad-hoc Constraint Execution</th>
                  <th colspan="6" style="background-color: #fff9e6;">Contextual Knowledge Adaptation</th>
                </tr>
                <tr style="background-color: #fafafa;">
                  <th colspan="3" style="background-color: #f4fbf7;"><i>Implicit Pattern</i></th>
                  <th colspan="3" style="background-color: #f4f9fd;"><i>Symbolic Constraint</i></th>
                  <th colspan="3" style="background-color: #f4f9fd;"><i>Visual Constraint</i></th>
                  <th colspan="3" style="background-color: #fffdf0;"><i>Prior-Conflicting</i></th>
                  <th colspan="3" style="background-color: #fffdf0;"><i>Multi-Semantic</i></th>
                </tr>
                <tr style="background-color: #f5f5f5;">
                  <th>RC</th><th>VC</th><th>AQ</th>
                  <th>RC</th><th>VC</th><th>AQ</th>
                  <th>RC</th><th>VC</th><th>AQ</th>
                  <th>RC</th><th>VC</th><th>AQ</th>
                  <th>RC</th><th>VC</th><th>AQ</th>
                </tr>
              </thead>
              <tbody>
                
                <tr style="background-color: #eef6fc; font-weight: bold; text-align: left;">
                  <td colspan="18">Proprietary Models</td>
                </tr>
                
                <tr>
                  <td style="text-align: left;">Nano Banana Pro</td>
                  <td>✅</td>
                  <td style="background-color: #ffe6e6;"><b>57.19</b></td> <td style="background-color: #e6f7ff;">66.86</td><td style="background-color: #e6f7ff;">44.59</td><td style="background-color: #e6f7ff;">96.51</td>
                  <td style="background-color: #ffe6e6;">71.38</td><td style="background-color: #e6f7ff;">50.00</td><td>92.11</td>
                  <td style="background-color: #ffe6e6;">76.67</td><td style="background-color: #e6f7ff;">66.67</td><td style="background-color: #ffe6e6;">96.67</td>
                  <td style="background-color: #ffe6e6;">52.97</td><td style="background-color: #ffe6e6;">41.38</td><td>90.59</td>
                  <td style="background-color: #ffe6e6;">35.45</td><td>-</td><td style="background-color: #ffe6e6;">95.00</td>
                </tr>
                <tr>
                  <td style="text-align: left;">Nano Banana</td>
                  <td>✅</td>
                  <td>50.66</td>
                  <td>56.47</td><td>39.04</td><td>94.12</td>
                  <td>60.46</td><td style="background-color: #ffe6e6;">51.91</td><td>90.20</td>
                  <td style="background-color: #e6f7ff;">68.33</td><td style="background-color: #ffe6e6;">79.17</td><td style="background-color: #e6f7ff;">93.33</td>
                  <td>35.50</td><td style="background-color: #e6f7ff;">39.47</td><td style="background-color: #e6f7ff;">91.00</td>
                  <td>30.28</td><td>-</td><td style="background-color: #e6f7ff;">93.12</td>
                </tr>
                <tr>
                  <td style="text-align: left;">GPT-Image</td>
                  <td>❌</td>
                  <td>47.15</td>
                  <td>58.14</td><td>41.92</td><td>93.60</td>
                  <td>58.82</td><td>32.82</td><td style="background-color: #e6f7ff;">93.79</td>
                  <td>49.17</td><td>62.50</td><td>92.50</td>
                  <td style="background-color: #e6f7ff;">43.50</td><td>33.33</td><td>90.00</td>
                  <td>28.64</td><td>-</td><td>85.45</td>
                </tr>
                <tr>
                  <td style="text-align: left;">SeeDream 4.0</td>
                  <td>❌</td>
                  <td>21.26</td>
                  <td>12.05</td><td>0.70</td><td>96.39</td>
                  <td>21.57</td><td>3.44</td><td>84.64</td>
                  <td>40.00</td><td>4.17</td><td>76.67</td>
                  <td>30.69</td><td>10.34</td><td>82.67</td>
                  <td>30.73</td><td>-</td><td>80.00</td>
                </tr>
                <tr>
                  <td style="text-align: left;">SeeDream 4.5</td>
                  <td>❌</td>
                  <td style="background-color: #e6f7ff;">52.84</td>
                  <td style="background-color: #ffe6e6;">70.00</td><td style="background-color: #ffe6e6;">59.59</td><td style="background-color: #ffe6e6;">97.06</td>
                  <td style="background-color: #e6f7ff;">62.91</td><td>41.09</td><td style="background-color: #ffe6e6;">94.37</td>
                  <td>58.33</td><td>62.50</td><td>86.67</td>
                  <td>40.10</td><td style="background-color: #ffe6e6;">41.38</td><td style="background-color: #ffe6e6;">92.57</td>
                  <td style="background-color: #e6f7ff;">35.00</td><td>-</td><td>86.82</td>
                </tr>

                <tr style="background-color: #eef6fc; font-weight: bold; text-align: left;">
                  <td colspan="18">Open-Source Models</td>
                </tr>

                <tr>
                  <td style="text-align: left;">Qwen-Image</td>
                  <td>❌</td>
                  <td>30.58</td>
                  <td>36.18</td><td>27.69</td><td>71.05</td>
                  <td>36.18</td><td>27.69</td><td>71.05</td>
                  <td>26.67</td><td>45.83</td><td>55.83</td>
                  <td>27.72</td><td>20.69</td><td>71.78</td>
                  <td>25.91</td><td>-</td><td>69.55</td>
                </tr>
                <tr>
                  <td style="text-align: left;">GLM-Image</td>
                  <td>❌</td>
                  <td>24.71</td>
                  <td>32.94</td><td>19.86</td><td>93.53</td>
                  <td>22.37</td><td>21.15</td><td>87.50</td>
                  <td>27.50</td><td>12.50</td><td>70.83</td>
                  <td>20.30</td><td>15.52</td><td>71.29</td>
                  <td>17.73</td><td>-</td><td>70.91</td>
                </tr>
                <tr>
                  <td style="text-align: left;">FLUX.2-dev</td>
                  <td>❌</td>
                  <td>34.39</td>
                  <td>34.30</td><td>27.70</td><td>88.95</td>
                  <td>35.76</td><td>31.01</td><td>87.09</td>
                  <td>39.17</td><td>50.00</td><td>59.17</td>
                  <td>25.25</td><td>30.17</td><td>84.16</td>
                  <td>29.82</td><td>-</td><td>79.82</td>
                </tr>
                <tr>
                  <td style="text-align: left;">NextStep-1</td>
                  <td>❌</td>
                  <td>10.44</td>
                  <td>10.74</td><td>0.40</td><td>25.12</td>
                  <td>11.33</td><td>2.54</td><td>21.67</td>
                  <td>21.50</td><td>4.20</td><td>29.17</td>
                  <td>15.49</td><td>7.55</td><td>28.71</td>
                  <td>12.80</td><td>-</td><td>20.28</td>
                </tr>
                <tr>
                  <td style="text-align: left;">Emu3.5-Image</td>
                  <td>❌</td>
                  <td>36.67</td>
                  <td>41.86</td><td>35.81</td><td>83.72</td>
                  <td>34.97</td><td>39.31</td><td>86.93</td>
                  <td>24.17</td><td>29.17</td><td>42.50</td>
                  <td>26.24</td><td>37.93</td><td>82.18</td>
                  <td>32.87</td><td>-</td><td>75.46</td>
                </tr>
                <tr>
                  <td style="text-align: left;">Omini-Gen2</td>
                  <td>❌</td>
                  <td>27.87</td>
                  <td>29.07</td><td>26.35</td><td>76.16</td>
                  <td>25.33</td><td>30.38</td><td>77.96</td>
                  <td>11.67</td><td>41.67</td><td>52.50</td>
                  <td>23.76</td><td>34.48</td><td>69.80</td>
                  <td>19.27</td><td>-</td><td>63.76</td>
                </tr>
                <tr>
                  <td style="text-align: left;">Bagel</td>
                  <td>✅</td>
                  <td>26.74</td>
                  <td>26.74</td><td>27.03</td><td>84.30</td>
                  <td>29.61</td><td>16.03</td><td>76.32</td>
                  <td>22.50</td><td>12.50</td><td>49.17</td>
                  <td>17.24</td><td>22.28</td><td>74.75</td>
                  <td>33.49</td><td>-</td><td>53.67</td>
                </tr>
                
                <tr style="background-color: #fffdf5; border: 2px solid #ffe08a;">
                  <td style="text-align: left;"><b>Ours</b></td>
                  <td>✅</td>
                  <td style="background-color: #e6f7ff;"><b>38.84</b></td> <td>47.34</td><td>48.51</td><td>71.88</td>
                  <td>40.71</td><td>30.81</td><td>70.02</td>
                  <td>32.77</td><td>38.56</td><td>50.51</td>
                  <td>28.78</td><td>40.87</td><td>62.96</td>
                  <td>34.81</td><td>-</td><td>56.81</td>
                </tr>
              </tbody>
            </table>

          </div>
          <h2 class="title is-3 has-text-left">Insights & Takeaways</h2>
          <ul>
            <li>
              <strong>Generative models face a severe bottleneck in fluid intelligence.</strong>
              Current models exhibit significant limitations in adaptability. 
              Even state-of-the-art models like Nano-Banana Pro fail to achieve a passing grade on the GENIUS benchmark, highlighting a critical gap in reasoning capabilities.
            </li>
            
            <li>
              <strong>Models struggle to transcend their pre-training distributions.</strong>
              Performance significantly degrades in Contextual Knowledge Adaptation tasks, where the context explicitly contradicts pre-trained priors. 
              This reveals a rigidity in current AI; models lack the cognitive flexibility to override the pre-trained priors and adapt to counter-intuitive rules, unlike human intelligence which demonstrates far more robust plasticity in updating beliefs.
            </li>
            <li>
               <strong>The fixation on aesthetics masks profound logical deficiencies.</strong> 
               Aesthetic scores consistently outstrip reasoning metrics, 
               reflecting a historical bias towards pixel-level fidelity (e.g., FID, IAA) 
               over logical correctness. Consequently,
                while models generate visually stunning images, 
                they frequently "lose the plot" when prompts require complex reasoning, 
                producing high-quality but semantically misaligned outputs.
            </li>
            <img src="./static/images/ablation_page-0001.jpg" alt="GENIUS Benchmark Overview" style="width:100%; height:auto; ">
            <li>
              <strong>Pre-Planning and Post-Reflection yield marginal gains.</strong> 
              As illustrated in (a), we explored inference-time strategies such as "Pre-Planning" 
              and "Post-Reflection" (feeding evaluated generations back as context). 
              However, these yielded only negligible improvements, 
              suggesting that current generic reasoning paradigms are insufficient 
              to address the specific complexities of this task.
            </li>
            <li>
              <strong>Context comprehension is the linchpin of success.</strong> 
              Injecting human-curated rubrics into the context resulted in substantial performance surges. 
              Since these rubrics distill human understanding, 
              this implies that if models could autonomously achieve similar comprehension, 
              the problem is largely solvable. 
              However, results remain bounded by intrinsic model capabilities; for instance, 
              Bagel's performance degradation even with 
              multimodal rubrics highlights existing weaknesses in processing complex, 
              interleaved multimodal inputs.
            </li>
            <li>
              <strong>Generative failure stems primarily from an execution gap, 
                not a comprehension deficit.</strong> 
                To test comprehension, we reformulated the task into a VQA format (b), 
                using the rubric as the Ground Truth. Surprisingly, 
                models achieved high accuracy, indicating a solid grasp of the context. 
                We attribute the persistent generation 
                failure—a "know-but-cannot-draw" phenomenon—to two factors: 
                the difficulty of fully articulating fine-grained visual nuances 
                within dense interleaved contexts, 
                and structural inefficiencies in current UMMs 
                where rich semantic understanding 
                from the encoder fails to effectively propagate to the generative decoder.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Building upon these findings, we conducted a deeper investigation:
          </p>
          <img src="./static/images/image_attn_page-0001.jpg" alt="GENIUS Benchmark Overview" style="width:100%; height:auto;">
          <p>
            We visualized the attention mechanism 
            by employing the generated image tokens 
            as Queries against the interleaved multimodal context (serving as Keys). 
            Observations reveal that Bagel's attention distribution is erratic, 
            characterized by irregular noise and stochastic spikes. 
            This raises a critical question: 
            To what extent does attention dictate context comprehension, 
            and can a simple modulation of these weights enhance model performance?
          </p>
          <img src="./static/images/method_page-0001.jpg" alt="GENIUS Benchmark Overview" style="width:100%; height:auto">
          <p>
            Drawing on theoretical frameworks from <sup><a href="#ref-11">[11]</a></sup> that interpret <strong>In-Context Learning (ICL) as implicit gradient descent</strong>, we extended this analysis to the Bagel architecture (full derivation provided in the paper). 
            Theoretically, high-quality context acts as a signal for a more precise and definitive "gradient descent" trajectory. 
            However, Bagel's diffuse attention maps suggest a failure to focus on critical task-relevant features. 
            Consequently, the implicit gradient update lacks a coherent descent direction, trapping the model within its pre-training priors.
          </p>
          
          <p>
            To address this, we propose a <strong>training-free attention adjustment mechanism</strong> (detailed in the paper) that redirects focus toward semantically relevant regions. 
            Both qualitative and quantitative experiments demonstrate consistent improvements, establishing a <strong>Strong Baseline</strong> for future research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="second-half-details">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        
        <div class="content has-text-justified">
          <p>
            As we transition into the next phase of UMM research, I want to offer several critical questions and insights that may define the path forward:
          </p>
          
          <ul>
            <li>
              <strong>Defining the ideal task.</strong> 
              What capabilities should an ideal UMM possess that distinguish it from standard generative models? We hope GENIUS serves as a signpost in this trajectory, delineating tasks that require genuine multimodal reasoning rather than mere fitting.
            </li>
            
            <li>
              <strong>The agentic challenge & The "existential crisis".</strong> 
              As "thinking with generated images" gains traction, we face a pivotal question: if specialized models (like Nano-Banana Pro) excel at generation, why must the intermediate imagery within a thinking trajectory originate from the UMM itself? This poses an existential challenge regarding the necessity of unified architectures versus modular, agentic systems.
            </li>
            
            <li>
              <strong>Synergy Between Understanding and Generation.</strong> 
              While it is consensus that understanding aids generation, the reverse remains an open question. However, from first principles—and as discussed in our NeurIPS 2025 paper—the act of creation should deepen comprehension, mirroring human cognition where sketching clarifies concepts. The hurdle remains finding the appropriate tasks and scenarios to manifest this loop.
            </li>
            
            <li>
              <strong>Architectural divergence.</strong> 
              Despite the exhaustive permutation of components, few models truly master interleaved inputs. The field may be suffering from path dependence on VLM architectures (like LLaVA). We must question: is the established path the correct one? This doubt is driving a shift toward native multimodal architectures in 2025.
            </li>
            
            <li>
              <strong>Visualizing understanding.</strong> 
              The current paradigm often constrains generation as a byproduct of understanding (e.g., LLMs performing denoising). Perhaps the reverse—adapting "understanding" to serve "generation"—aligns better with human perception? Recent explorations like DeepSeek-OCR, which utilize visual modalities to structure understanding, point toward this possibility.
            </li>
            
            <li>
              <strong>The object of unification.</strong> 
              Text, with its sequential and causal nature, shares more with video than with static images. Unifying static image generation with language understanding might be a suboptimal local minimum; a shift toward video generation/understanding could resolve many current architectural dissonances.
            </li>
            
            <li>
              <strong>Latent vs. Pixel-Level.</strong> 
              For internal reasoning (excluding human interaction), generating at the latent level might suffice to aid understanding, rendering pixel-level reconstruction redundant and noisy. The critical question for the "second half" is not if modalities can share a latent space, but specifically what kind of shared latent space best supports reasoning.
            </li>
            
            <li>
              <strong>The evolution of generative models.</strong> 
              I boldly predict the end of standalone diffusion models. The future likely belongs to integrated systems combining Model Context Protocol (MCP) and agentic workflows.
            </li>
          </ul>

          <hr>

          <p>
            The first half of the UMM era was a carnival of memorization and fitting. 
            The second half will be an arduous journey toward reasoning, adaptation, and real-world grounding.
            To discuss fluid intelligence is to discuss the evolution of UMMs from skilled "rote memorizers" into genuine "thinkers." 
            We must aim to redefine the intellectual boundaries of UMMs, moving beyond the simple superposition of understanding and generation tasks.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="references">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <div style="font-size: 0.8em; line-height: 1.6;">
      
      <p id="ref-1">
        [1] Chameleon: Mixed-modal early-fusion foundation models
      </p>
      
      <p id="ref-2">
        [2] Show-o: One single transformer to unify multimodal understanding and generation
      </p>
      
      <p id="ref-3">
        [3] Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
      </p>
      
      <p id="ref-4">
        [4] Emu: Generative Pretraining in Multimodality
      </p>
      
      <p id="ref-5">
        [5] Emerging Properties in Unified Multimodal Pretraining
      </p>
      
      <p id="ref-6">
        [6] Theory of fluid and crystallized intelligence: A critical experiment
      </p>
      
      <p id="ref-7">
        [7] On the nature of crystallized intelligence: The relationship between verbal ability and factual knowledge
      </p>
      
      <p id="ref-8">
        [8] Fluid intelligence: A brief history
      </p>
      
      <p id="ref-9">
        [9] CL-bench: A Benchmark for Context Learning
      </p>
      
      <p id="ref-10">
        [10] ARC Prize 2024: Technical Report
      </p>
      
      <p id="ref-11">
        [11] Learning without training: The implicit dynamics of in-context learning
      </p>

    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a <a rel="license"
                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>


